{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files\n",
    "def load_file(filename):\n",
    "    file = open(filename, encoding=\"utf8\")\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean file\n",
    "def clean_file(file):\n",
    "    # splitting into tokens\n",
    "    tokens = file.split()\n",
    "    # filter character\n",
    "    punc = re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    # remove punctuation\n",
    "    tokens = [punc.sub(' ', w) for w in tokens]\n",
    "    # remove non-alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter other\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add file to vocab\n",
    "def add_file_to_vocab(filename, vocab):\n",
    "    file = load_file(filename)\n",
    "    tokens = clean_file(file)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all file from a folder\n",
    "def process_files(folder, vocab):\n",
    "    for filename in listdir(folder):\n",
    "        path = folder + '/' + filename\n",
    "        add_file_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list\n",
    "def save_list(lines, filename):\n",
    "    # convert line to text\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w', encoding=\"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokens\n",
    "vocab = load_file('vocab2.txt')\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cleaning involves filtering out\n",
    "# all the tokens not in the vocabulary\n",
    "def clean_file_vocab(file, vocab):\n",
    "    tokens = file.split()\n",
    "    punc = re.compile('[%s]' %re.escape(string.punctuation))\n",
    "    tokens = [punc.sub('', w) for w in tokens]\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_vocab(folder, vocab):\n",
    "    documents = list()\n",
    "    for filename in listdir(folder):\n",
    "        path = folder + '/' + filename\n",
    "        file = load_file(path)\n",
    "        tokens = clean_file_vocab(file, vocab)\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean dataset for training\n",
    "def clean_dataset(vocab, left_data, leanleft_data, centre_data, leanright_data, right_data):\n",
    "    # load files\n",
    "    left = process_files_vocab(left_data, vocab)\n",
    "    leanleft = process_files_vocab(leanleft_data, vocab)\n",
    "    centre = process_files_vocab(centre_data, vocab)\n",
    "    leanright = process_files_vocab(leanright_data, vocab)\n",
    "    right = process_files_vocab(right_data, vocab)\n",
    "    \n",
    "    files = left + leanleft + centre + leanright + right\n",
    "    # labelling dataset\n",
    "    # taken help from here: https://nlp.stanford.edu/IR-book/html/htmledition/classification-with-more-than-two-classes-1.html\n",
    "    \n",
    "    labels = array([[1, 0, 0, 0, 0] for _ in range(len(left))] + [[0, 1, 0, 0, 0] for _ in range(len(leanleft))] \n",
    "                   + [[0, 0, 1, 0, 0] for _ in range(len(centre))] + [[0, 0, 0, 1, 0] for _ in range(len(leanright))]\n",
    "                   + [[0, 0, 0, 0, 1] for _ in range(len(right))])\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# necessary modules to tokenize the text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting tokenizer on texts\n",
    "def new_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode documents as integer and pad documents\n",
    "def encode_and_pad(tokenizer, max_length, files):\n",
    "    # encoding\n",
    "    encoded = tokenizer.texts_to_sequences(files)\n",
    "    # padding\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got this while training\n",
    "vocab_size = 91679\n",
    "max_length = 8346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files, ytest = clean_dataset(vocab, 'data/test/left', 'data/test/leanleft', \n",
    "                                    'data/test/centre', 'data/test/leanright', 'data/test/right')\n",
    "tokenizer_test = new_tokenizer(test_files)\n",
    "Xtest = encode_and_pad(tokenizer_test, max_length, test_files)\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 17:15:32.016240 33784 deprecation_wrapper.py:119] From C:\\Users\\Vedansh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0809 17:15:35.036723 33784 deprecation_wrapper.py:119] From C:\\Users\\Vedansh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data when using Val Data:  61.31669714536275\n"
     ]
    }
   ],
   "source": [
    "modelv1 = load_model('sequential_10epochv1.h5')\n",
    "acc, _ = modelv1.evaluate(Xtest, ytest, verbose=0)\n",
    "print(\"Accuracy on Test Data when using Val Data: \", acc*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data:  74.35901250447586\n"
     ]
    }
   ],
   "source": [
    "modelv2 = load_model('sequential_10epoch.h5')\n",
    "acc, _ = modelv2.evaluate(Xtest, ytest, verbose=0)\n",
    "print(\"Accuracy on Test Data: \", acc*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bias(text, vocab, tokenizer, max_length, model):\n",
    "    line = clean_file_vocab(text, vocab)\n",
    "    padded = encode_and_pad(tokenizer, max_length, [line])\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    ideologies = [\"Left\", \"Lean Left\", \"Center\", \"Lean Right\", \"Right\"]\n",
    "    index = np.argmax(yhat)\n",
    "    print(ideologies[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lean Right\n"
     ]
    }
   ],
   "source": [
    "# Trump's recent Tweet\n",
    "text = 'Sleepy Joe Biden just agreed with the Radical Left Democrats to raise Taxes by Three Trillion Dollars. Everyone will pay - Will kill your Stocks, 401k’s, and the ECONOMY. BIG CRASH!'\n",
    "review = predict_bias(text, vocab, tokenizer_test, max_length, modelv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center\n"
     ]
    }
   ],
   "source": [
    "# Snippet of NYT article\n",
    "text2 = 'President Trump took executive action on Saturday to circumvent Congress and try to extend an array of federal pandemic relief, resorting to a legally dubious set of edicts whose impact was unclear, as negotiations over an economic recovery package appeared on the brink of collapse.It was not clear what authority Mr. Trump had to act on his own on the measures or what immediate effect, if any, they would have, given that Congress controls federal spending. But his decision to sign the measures — billed as a federal eviction ban, a payroll tax suspension, and relief for student borrowers and $400 a week for the unemployed — reflected the failure of two weeks of talks between White House officials and top congressional Democrats to strike a deal on a broad relief plan as crucial benefits have expired with no resolution in sight.Mr. Trump’s move also illustrated the heightened concern of a president staring down re-election in the middle of a historic recession and a pandemic, and determined to show voters that he was doing something to address the crises. But despite Mr. Trump’s assertions on Saturday that his actions “will take care of this entire situation,” the orders also leave a number of critical bipartisan funding proposals unaddressed, including providing assistance to small businesses, billions of dollars to schools ahead of the new school year, aid to states and cities and a second round of $1,200 stimulus checks to Americans.“Nancy Pelosi and Chuck Schumer have chosen to hold this vital assistance hostage,” Mr. Trump said, savaging the two top Democrats and their $3.4 trillion opening offer during a news conference at his private golf club in New Jersey, his second in two days. A few dozen club guests were in attendance, and the president appeared to revel in their laughter at his jokes denouncing his political rivals.'\n",
    "review = predict_bias(text2, vocab, tokenizer_test, max_length, modelv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lean Right\n"
     ]
    }
   ],
   "source": [
    "# Some Fun\n",
    "text3 = \"Vedansh's idology?\"\n",
    "review = predict_bias(text3, vocab, tokenizer_test, max_length, modelv2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
